# -*- coding: utf-8 -*-
"""[LINTASARTA]_Proyek_Pertama_Membuat_Model_NLP_offline_Python

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sKt8a20FZgJ8reWP46F4j_jpGexbYKQV

SUMBER DATASET: https://www.kaggle.com/datasets/yufengdev/bbc-fulltext-and-category

REFERENSI CODE: https://www.kaggle.com/code/muhammadnss/klasifikasi-teks-berita-bbc
"""

from google.colab import drive
drive.mount('/content/drive')

import os
path = '/content/drive/My Drive/LINTASARTA/'
os.listdir(path)

import pandas as pd
df = pd.read_csv(path + 'bbc-text.csv', encoding='utf-8')
#df = pd.read_csv('bbc-text.csv', encoding='utf-8')
df.head()

df.info()

#df = pd.read_csv('/kaggle/input/bbc-fulltext-and-category/bbc-text.csv')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import itertools
import os
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from sklearn.metrics import confusion_matrix
from tensorflow import keras
layers = keras.layers
models = keras.models

# Pada program ini menggunakan TensorFlow v1.8
print("Anda menggunakan tensorflow versi : ", tf.__version__)

import seaborn as sns
result = df.groupby(['category']).size()
#plot the result
sns.barplot(x=result.index, y=result.values)

"""## STOPWORDS"""

import pandas as pd
import nltk
from nltk.corpus import stopwords

# Pastikan Anda sudah mendownload corpus stopwords untuk bahasa Inggris
nltk.download('stopwords')

# Ambil daftar stopwords bahasa Inggris
english_stopwords = set(stopwords.words('english'))

# Function untuk membersihkan stopwords
def remove_stopwords(text):
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in english_stopwords]
    return ' '.join(filtered_words)

df['text'] = df['text'].apply(remove_stopwords)

"""## One-Hot-Encoding"""

kategori = pd.get_dummies(df.category)
df_baru = pd.concat([df, kategori], axis=1)
df_baru = df_baru.drop(columns='category')
df_baru

"""## mengubah nilai-nilai dari dataframe ke dalam tipe data numpy array menggunakan atribut values"""

teks = df_baru['text'].values
label = df_baru[['business', 'entertainment', 'politics', 'sport', 'tech']].values

"""## bagi data untuk training dan data untuk testing"""

from sklearn.model_selection import train_test_split
teks_latih, teks_test, label_latih, label_test = train_test_split(teks, label, test_size=0.2)

"""## ubah setiap kata ke dalam bilangan numerik dengan fungsi Tokenizer"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(teks_latih)
tokenizer.fit_on_texts(teks_test)

sekuens_latih = tokenizer.texts_to_sequences(teks_latih)
sekuens_test = tokenizer.texts_to_sequences(teks_test)

padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)

"""- digunakan layer Embedding dengan dimensi embedding sebesar 16
- dimensi dari input sebesar nilai num_words pada objek tokenizer
- memanggil fungsi compile dan menentukan optimizer serta loss function
"""

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    tf.keras.layers.Dropout(0.2), # Mengurangi dropout rate
    tf.keras.layers.Dense(64, activation='relu'),
    #tf.keras.layers.Dense(128, activation='relu'),
    #tf.keras.layers.Dropout(0.2), # Mengurangi dropout rate
    #tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    #tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2), # Mengurangi dropout rate
    tf.keras.layers.Dense(5, activation='softmax')
])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

"""membuat kelas callback"""

#class myCallback(tf.keras.callbacks.Callback):
#  def on_epoch_end(self, epoch, logs={}):
#    if logs.get('accuracy') is not None and logs.get('accuracy') > 0.9:
#      print("\nAkurasi telah mencapai >90%!")
#      self.model.stop_training = True

#callbacks = myCallback()  # Instantiate the callback directly

# utils
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping

import tensorflow as tf

#Callback Function
class accCallback(Callback):
   def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') >= 0.9 and logs.get('val_accuracy') >= 0.9):
            print("\nAccuracy and Val_Accuracy has reached 90%!", "\nEpoch: ", epoch)
            self.model.stop_training = True

callbacks = accCallback()  # Use the correct class name

auto_reduction_LR = ReduceLROnPlateau(
    monitor = 'val_accuracy',
    patience = 2, #setelah 2 epoch, jika tidak ada kenaikan maka LR berkurang
    verbose = 1,
    factor = 0.1,
    min_lr = 0.001
)

auto_stop_learn = EarlyStopping(
    monitor = 'val_accuracy',
    min_delta = 0,
    patience = 4,
    verbose = 1,
    mode = 'auto'
)

"""## melatih model dengan memanggil fungsi fit()"""

history = model.fit(padded_latih, label_latih,
                    epochs = 50, #50
                    batch_size = 32, #64
                    steps_per_epoch = 30,
                    validation_data = (padded_test, label_test),
                    verbose = 1,
                    callbacks=[callbacks, auto_reduction_LR, auto_stop_learn],
                    )

"""Plot akurasi model"""

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Plot loss"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()